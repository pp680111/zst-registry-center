* 在服务端记录版本号，每次服务更新数据之后版本号都+1，后续客户端可以用自己的版本号来获取增量变化，
* 客户端在首次全量获取时，初始化版本号
* 版本号查的太多（超出阈值）可以直接全量获取

DONE:
* CLuster的server列表需要每个实例进行互相探活，使用http接口的方式，对外暴露一个http接口，返回当前Server信息，将返回的server信息写入到当前实例持有的Server列表中
* 由于cluster的探活中调用的接口获取的server对象中的isLeader状态在选举时没有被更新，这里需要处理一下，要不就让currentServer对象放到server中，或者与server中的同一个引用，要不就选举时如果leader节点的地址等于当前节点的话，更新以下当前server的状态（前提是当前节点的ip需要能对的上serverlist中配置的当前节点的address）
* 实现简单的选举算法，当本实例中没有一个存活的leader节点时，或者有多个leader节点，从存活的节点中挑选一个hashCode最小的节点作为leader节点
* 把ClusterHealthChecker里面对其他服务节点发起http调用的地方改成并行的调用（比如用httpasyncclient的reactor异步模式），避免单个节点的故障影响其他节点的探活
* ClusterHealthChecker探活时可以排除掉serverList里面自己这个节点
* 实现从主节点同步节点列表数据
  * 如果自己是主节点的话，skip （ok）
  * 用全量同步的方式，从主节点拉取数据，覆盖自己节点的数据（这里是不是可以加一个判断，如果变更了主节点才做全量，否则就根据版本来做增量就好了）,然后更新自己接点的version （ok）
  * 写一个接口，触发当前实例中全量数据的快照生成（这里是否要注意加一个全局锁，避免在同步实例列表的时候，被客户端修改了节点数据？或者因为自己不是主节点，从节点不允许写，所以不考虑这个问题）（ok）
  * 在接口中变更注册实例信息时，如果自己不是leader节点，那么要拒绝（ok）
* 思考一下，主节点或从节点挂了之后重新加入，怎么正确识别主节点，然后恢复数据(目前的选举算法的选举结果是固定的，因此这部分的问题不大)
* 写一个获取当前cluster所有节点的接口，客户端可以通过配置一个或少数个节点的地址，然后通过http接口获取到当前cluster中所有的注册中心的节点信息列表
* 从节点的数据修改接口调用，需要报错

TODO:
* 尝试用gossip算法实现无主节点的模式
* 代码结构还比较乱，需要整理一下
* 试一下使用强外部分布式锁的方式来处理主节点的选举
* 重构一下实例变更数据存储方式，尝试使用事件存储的思维来处理，以事件堆叠出一个视图，一个事件一个version，太长的部分就定时压缩
* 建立一个client库，提供给之前做的rpc项目集成
  * 客户端需要记录version，根据server端的version是否变化来判断吧是否执行fetchAll
  * rpc那边写一个adapter类，用来适配这个register项目，然后由rpc-core引入此依赖,adapter类要实现原有的RegistryCenter接口的所有方法，然后通过spring的自动配置机制声明为bean
  * 把原本的zkRegistryCenter也拆成adapter库
  * 把注册中心的获取版本的请求改成那种会会挂起一段时间的长轮询请求，在等待时间内如果版本发生了变化，那么就返回新的版本，可以有效节省网络通信开销
* renew接口运行时发现instanceMap里面没有数据，要检查下为什么